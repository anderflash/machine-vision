\documentclass[9pt, twocolumn]{extarticle}
\usepackage{lipsum}
\usepackage[top=1cm, bottom=1.75cm, left=0.8cm, right=0.8cm]{geometry}
\usepackage{titlesec}
\usepackage{titling}
\usepackage{fourier}
\usepackage{multicol}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{amsmath, amsthm}
\usepackage[framemethod=tikz]{mdframed}

\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{nosep,noitemsep, topsep=0pt}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\title{Time of Flight Cameras}
\author{Anderson Tavares, anderson.moreira.tavares@liu.se}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%\surroundwithmdframed[hidealllines=true,backgroundcolor=gray!30]{theorem}
%\surroundwithmdframed[hidealllines=true, backgroundcolor=cyan!10, leftmargin=0cm, innerleftmargin=0.1cm, skipabove=0cm, skipbelow=0cm innerrightmargin=1.6cm, innertopmargin=0.1cm, innerbottommargin=0.1cm]{definition}
%\surroundwithmdframed[backgroundcolor=red,backgroundcolor=orange!30,roundcorner=10pt]{example}


%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\headsep}{0pt}
%\setlength{\topskip}{0pt}
%\setlength{\topmargin}{0pt}
%\setlength{\topsep}{0pt}
%\setlength{\partopsep}{0pt}
\setlength\abovedisplayskip{0pt}
%\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\droptitle}{-12em}   % This is your set screw
\titlespacing*{\section}
{0pt}{1.5ex plus 1ex minus .2ex}{0.0ex plus .0ex}
\titlespacing*{\subsection}
{0pt}{5.5ex plus 1ex minus .2ex}{0.0ex plus .0ex}
\renewcommand{\baselinestretch}{0.9}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}

  
\newcommand{\norm}[1]{\left\Vert #1\right\Vert}

\begin{document}
  \twocolumn[
  \begin{@twocolumnfalse}
    \noindent\centering\parbox{\textwidth}{%
      \centering {\bfseries\fontsize{14}{16}\selectfont\thetitle}\\{\fontsize{12}{14}\selectfont\theauthor}
%     \parbox{.6\linewidth}{\centering\bfseries\fontsize{14}{16}\selectfont\thetitle}\hfill%
%      \parbox{.4\linewidth}{\fontsize{12}{14}\selectfont\raggedleft\today\\\theauthor}%
  }
  
  \end{@twocolumnfalse}
  ]
  \section{Foundations of Signal Processing}
  \subsection{From Euclid to Hilbert}
  \subsubsection{Vector Spaces}
  
  \begin{definition}{(\textsc{Field})} 
    Let $ F $ be a set. An \emph{operation} is a mapping $ f:F\times F \rightarrow F$. Let \emph{addition} and \emph{multiplication} be the operations $ a + b$ and $ a\cdot b, \forall a,b\in F$. Let $ c \in F $. $ F $ is a \emph{field} if it has these properties:
    \begin{enumerate}[label=(\roman*)]
      \item \emph{Associativity of addition and multiplication}: $ (a+b)+c=a+(b+c) $ and $ (a\cdot b)\cdot c=a\cdot (b\cdot c) $
      
      \item \emph{Commutativity of add. and mult.}: $ a+b=b+a $ and $ a\cdot b=b\cdot a $
      \item \emph{Additive and multiplicative identity}: $ \exists 0, 1\in F: a+0=a $ and $ a\cdot 1=a $
      \item \emph{Additive inverses}: $\forall a\in F, \exists -a\in F: a+(-a)=0$
      \item \emph{Multiplicative inverses}: $\forall a\neq 0 \in F, \exists a^{-1}(\text{ or } 1/a)\in F: a\cdot a^{-1}=1$.
      \item \emph{Distributivity of multiplication over addition}: $ a\cdot(b+c) = (a\cdot b) + (a\cdot c) $.      
    \end{enumerate}    
  \end{definition}

  \begin{definition}{(\textsc{Vector Space})}
    A \emph{vector space} over a field of scalars $ \mathds{C} $ (or $ \mathds{R} $) is a set of vectors, $ V$, together with operations of vector addition and scalar multiplication. For any $x$, $y$, $z$ in $ V $ and $ \alpha $, $ \beta $ in $ \mathds{C} $ (or $ \mathds{R} $), these operations must satisfy the following properties:
    \begin{enumerate}[label=(\roman*)]
      \item \emph{Commutativity}: $ x + y=y+x $.
      \item \emph{Associativity}: $ (x+y)+z=x+(y+z) $ and $ (\alpha\beta)x=\alpha(\beta x) $.
      \item \emph{Distributivity}: $ \alpha(x+y)= \alpha x + \alpha y $ and $ (\alpha+\beta)x=\alpha x + \beta x .$
      \item \emph{Additive identity}: $\forall x\in V, \exists \textbf{0} \in V : x + \textbf{0} = \textbf{0} + x = x$.
      \item \emph{Additive inverse}: $ \forall x\in V, \exists -x\in V: x+(-x)=(-x)+x=\textbf{0} $
      \item \emph{Multiplicative identity}: $ \forall x\in V, 1\cdot x = x$
    \end{enumerate}
  \end{definition}

\noindent$ \mathds{C}^N $: \textbf{Vector space of complex-valued finite-dimensional vectors}\vspace{-0.18cm}
\[ \mathds{C}^N = \left\{x=\begin{bmatrix}x_0&x_1&\dots&x_{N-1}\end{bmatrix}^T \middle| x_n\in \mathds{C}, n\in \{0,1,\dots,N-1\}\right\} \]

\noindent$\mathds{C}^\mathds{Z}$: \textbf{Vector space of complex-valued sequences over $ \mathds{Z} $}\vspace{-0.18cm}
\[ \mathds{C}^\mathds{Z} = \left\{x=\begin{bmatrix}\dots&x_{-1}&x_0&x_1&\dots\end{bmatrix}^T \middle| x_n\in \mathds{C}, n\in \mathds{Z}\right\} \]

\noindent$\mathds{C}^\mathds{R}$: \textbf{Vector space of complex-valued function over $ \mathds{R} $}\vspace{-0.18cm}
\[ \mathds{C}^\mathds{R} = \left\{x \middle| x(t)\in \mathds{C}, t \in \mathds{R}\right\}\]\vspace{-0.5cm}\[(x+y)(t)=x(t)+y(t) \text{ and } (\alpha x)(t) = \alpha x(t) \]

\noindent \textbf{Vector Space of Polynomials}: since each polynomial $x(t)$ is specified by its coefficients, they combine exactly like vectors in $ \mathds{R}^N $:\vspace{-0.3cm}
\[x(t) = \sum_{k=0}^{N-1}\alpha_kt^k\]

\begin{definition}{(\textsc{Subspace})}
  A nonempty subset $S$ of a vector space $V$ is a subspace when it is closed under the operations of vector addition and scalar multiplication: (i) $\forall x,y \in S, x+y\in S$ and (ii) $\forall x \in S \text{ and } \alpha\in \mathds{C}(\text{ or }\mathds{R}), \alpha x \in S$.
\end{definition}

\begin{itemize}
  \item Let $V$ be a vector space and $x\in V$. The set $\alpha x$ is a subspace ($ \alpha\in\mathds{C} $).
  \item In $ \mathds{C}^\mathds{Z} $, sequences with 0 outside indices $ \{2,3,4,5\} $ form a subspace.
  \item In $ \mathds{R}^\mathds{R} $, the functions that are constant on intervals $ \left[k-\frac{1}{2},k+\frac{1}{2}\right), k\in\mathds{Z} $, forms a subspace. Addition and multiplication by scalar does not affect the contant property.
  \item In the vector space of real valued functions on the interval $ \left[-\frac{1}{2},\frac{1}{2}\right] $,\\ $S_{odd}=\left\{x\middle|x(t)=-x(-t)\forall t\in\left[-\frac{1}{2},\frac{1}{2}\right]\right\}$ and\\ $S_{even}=\left\{x\middle|x(t)=x(-t)\forall t\in\left[-\frac{1}{2},\frac{1}{2}\right]\right\}$ are subspaces.
\end{itemize}

\begin{definition}{(\textsc{Convex Set})}
  In a \emph{convex set} $A$, if $x\in A$ and $y\in A$, so is $\lambda x + (1-\lambda)y, \lambda \in [0,1]$.
\end{definition}

\begin{definition}{(\textsc{Affine Subspace})}
  A subset $T$ of a vector $V$ is an \emph{affine subspace} when there is a subspace $S\subset V$ and $v\in V$ s.t. any $ t\in T $ can be written as $x+s$ for some $s\in S$. $T$ is a subspace if $\textbf{0}\in T$. Affine subspaces are convex sets.
\end{definition}

\begin{itemize}
  \item Let $ x\in V $ and $ y\in V $. The set $x+\alpha y$ is an affine space.
  \item In $ \mathds{C}^\mathds{Z} $, sequences that equal 1 outside $ \{2, 3, 4, 5\}$ is an affine subspace.
\end{itemize}

\begin{definition}{(\textsc{Span})}
  The \emph{span} is the set of all linear combinations: $$\text{span}(S)=\left\{\sum_{k=0}^{N-1}\alpha_k\phi_k\middle|\alpha_k\in \mathds{C} (\text{ or } \mathds{R}), \phi_k \in S \text{ and } N \in \mathds{N} \right\}$$
\end{definition}

\noindent\textbf{Proper Subspace}: 

\begin{definition}{(\textsc{Linearly Independent Set})}
  The set $ \left\{\phi_0, \phi_1, \dots,\phi_{N-1}\right\} $ is \emph{linear independent} when $\sum_{k=0}^{N-1}\alpha_k\phi_k=\textbf{0}$ is true only if $\alpha_k=0, \forall k$.
\end{definition}
\begin{definition}{(\textsc{Dimension})}
  A vector space $ V $ is said to have \emph{dimension} $ N $ when it contains a linearly independent set with $ N $ elements and every set with $ N+1 $ or more elements is linearly dependent. If no such finite $ N $ exists, $ V $ is infinite-dimensional.
\end{definition}

% Put examples of dimension

\begin{definition}{(\textsc{Inner product})}
  An inner product on a vector space $ V $ over $ \mathds{C} $ (or $ \mathds{R} $) is a complex-valued (or real-valued) function $ \langle\cdot,\cdot\rangle $ defined on $ V\times V $, with the following properties $ \forall x,y,z \in V $ and $ \alpha \in \mathds{C} $ (or $ \mathds{R} $):
  \begin{enumerate}[label=(\roman*)]
    \item \emph{Distributivity}: $ \langle x+y, z\rangle = \langle x,z\rangle $.
    \item \emph{Linearity in the first argument}: $ \langle \alpha x, y\rangle = \alpha\langle x,y\rangle $.
    \item \emph{Hermitian symmetry}: $ \langle x,y\rangle^*= \langle y,x\rangle $.
    \item \emph{Positive definiteness}: $ \langle x,x\rangle \geq 0$, and $ \langle x,x\rangle = 0 $ iif $ x = \textbf{0} $.
  \end{enumerate}
\end{definition}

\begin{definition}{(\textsc{Orthogonality})}
  \begin{enumerate}[label=(\roman*)]
    \item Vectors $ x $ and $ y $ are said to \emph{orthogonal} when $ \langle x,y\rangle = 0 $, written as $ x\perp y $.
    \item A set of vectors $ S $ is called \emph{orthogonal} when $ x\perp y, \forall x,y\in S: x\neq y $.
    \item A set $ S $ is \emph{orthonormal} when it is orthogonal and $ \langle x,x\rangle = 1, \forall x\in S $.
    \item A vector $ x $ is \emph{orthogonal} to a set $ S $ when $ X\perp s, \forall s\in S $, written as $ x\perp S $.
    \item Sets $ S_0 $ and $ S_1 $ are \emph{orthogonal} if $ s_0\perp S_1, \forall s_0\in S_0 $, written as $ S_0\perp S_1 $.
    \item Given a subspace $ S $ of a vector space $ V $, the \emph{orthogonal complement} of S, denoted $ S^\perp $, is the set $ \left\{ x\in V\middle|x\perp S\right\} $ .
  \end{enumerate}
\end{definition}

\begin{definition}{(\textsc{Norm})}
  A \emph{norm} of a vector space $ V $ over $ \mathds{C} $ (or $ \mathds{R} $) is a real valued function $ \norm{\cdot} $ defined on $ V $, with the following properties $ \forall x,y\in V $ and $ \alpha\in\mathds{C} $ (or $ \mathds{R} $):
  \begin{enumerate}[label=(\roman*)]
    \item \emph{Positive definiteness}: $ \norm{x} \geq 0 $, and $ \norm{x} = 0 $ if and only if $ x = \textbf{0} $.
    \item \emph{Positive scalability}: $ \norm{\alpha x} = |\alpha|\norm{x} $
    \item \emph{Triangle inequality}: $ \norm{x+y}\leq \norm{x} + \norm{y} $, with equality iif $ y=\alpha x $.
  \end{enumerate}
\end{definition}

A vector space equipped with a norm becomes a \emph{normed vector space}. 

\begin{definition}{(\textsc{Metric, or distance})}
In a normed vector space, the \emph{metric}, or \emph{distance}, between vectors $ x $ and $ y $ is: $ d(x,y)=\norm{x-y} $.
\end{definition}

\begin{definition}{($ \ell^p(\mathds{Z}) $)}
  For any $ p\in[1,\infty]$, the normed vector space $\ell^p(\mathds{Z})$ is the subspace of $ \mathds{C}^\mathds{Z}  $ consisting of vectors with finite $ \ell^p $ norm.
\end{definition}

\begin{definition}{($ \pazocal{L}^p(\mathds{R}) $)}
  For any $ p\in[1,\infty]$, the normed vector space $\pazocal{L}^p(\mathds{R})$ is the subspace of $ \mathds{C}^\mathds{R} $ consisting of vectors with finite $ \pazocal{L}^p $ norm.
\end{definition}

\begin{definition}{(\textsc{Convergent sequence of vectors})}
  A sequence of vectors $ x_0, x_1, \cdots $ in a normed vector space $ V $ is said to \emph{converge} to $ v\in V $ when $ \lim\limits_{k\rightarrow\infty}\norm{v-x_k} = 0 $, that is, given any $ \varepsilon > 0 $, there exists a $ K_\varepsilon $ such that $ \norm{v-x_k} < \varepsilon, \forall k > K_\varepsilon $.
\end{definition}

\begin{definition}{(\textsc{Closed subspace})}
  A subspace $ S $ of a normed vector space $ V $ is \emph{closed} when it contains all limits of sequences of vectors in $ S $.
\end{definition}

\begin{definition}{(\textsc{Cauchy sequence of vectors})}
  A sequence of vectors $ x_0, x_1, \cdots $ in a normed vector space is a \emph{Cauchy sequence} when, given any $ \varepsilon > 0 $, there exists a $ K_\varepsilon $ such that $ \norm{x_k-x_m} < \varepsilon, \forall k, m > K_\varepsilon $.
\end{definition}

\begin{definition}{(\textsc{Completeness and Hilbert space})}
  A normed vector space $ V $ is \emph{complete} when every Cauchy sequence in $ V $ converges to a vector in $ V $. A complete inner product space is a \emph{Hilbert space}.
\end{definition}

\begin{definition}{(\textsc{Linear operator})}
  A function $ A:H_0\rightarrow H_1 $ is a \emph{linear operator} from $ H_0 $ to $ H_1 $ when, $ \forall x,y \in H_0 $ and $ \alpha\in\mathds{C} $ (or $ \mathds{R} $), the following hold:
  \begin{enumerate}[label=(\roman*)]
    \item \emph{Additivity}: $ A(x+y) = Ax + Ay$.
    \item \emph{Scalability}: $ A(\alpha x) = \alpha(Ax)$
  \end{enumerate}
  When $ H_0 = H_1 $, $ A $ is also called a linear operator on $ H_0 $.
\end{definition}

\begin{definition}{(\textsc{Operator norm and bounded linear operator})}
  The \emph{operator norm} of A, denoted by $ \norm{A} $, is defined as $ \norm{A} = \sup_{\norm{x}=1} \norm{A_x} $. A linear operator is \emph{bounded} when its operator norm is finite.
\end{definition}

\begin{definition}{(\textsc{Inverse})}
  A bounded linear operator $ A:H_0\rightarrow H_1 $ is \emph{invertible} if there is a bounded linear operator $ B: H_1\rightarrow H_0 $ such that $ BAx=x, \forall x\in H_0 \inlineeqnum\label{e:left_inverse}$ and $ ABy = y, \forall y\in H_1 \inlineeqnum\label{e:right_inverse}$. When such a $ B $ exists, it is unique, is denoted by $ A^{-1} $, and is called the \emph{inverse} of $ A $; $ B $ is a \emph{left inverse} of $ A $ when  (\ref{e:left_inverse}) holds, and $ B $ is a \emph{right inverse} of $ A $ when (\ref{e:right_inverse}) holds.
\end{definition}

\begin{definition}{(\textsc{Adjoint and Self-Adjoint Operator})}
  The linear operator $ A^*: H_1\rightarrow H_0 $ is the \emph{adjoint} of the linear operator $ A^*: H_0\rightarrow H_1 $ when $ \langle Ax, y\rangle_{H_1} = \langle x, A^*y\rangle_{H_0}, \forall x\in H_0$ and $ \forall y\in H_1 $. When $ A=A^* $, the operator $ A $ is called \emph{self-adjoint} or \emph{Hermitian}.
\end{definition}

\begin{theorem}{(\textsc{Adjoint properties})}
  Let $ A^*: H_0\rightarrow H_1 $ be a bounded linear operator.
  \begin{enumerate}[label=(\roman*)]
    \item The adjoint $ A^* $, defined through , exists.
    \item The adjoint $ A^* $ is unique.
    \item The adjoint of $ A^* $ equals the original operator, $ (A^*)^* = A $.
    \item The operators $ AA^* $ and $ A^*A $ are self-adjoint.
    \item The operator norms of $ A $ and $ A^* $ are equal, $ \norm{A^*} = \norm{A} $.
    \item If $ A $ is invertible, $ (A^{-1})^* = (A^*)^{-1} $.
    \item Let $ B:H_0\rightarrow H_1 $ be a bounded linear operator. Then, $ (A+B)^* = A^*+B^* $.
    \item Let $ B:H_0\rightarrow H_1 $ be a bounded linear operator. Then, $ (BA)^* = A^*B^* $.
  \end{enumerate}
\end{theorem}



  
%  \textbf{Real plane as a vector space}\\
  Let $x\in\mathds{R}^2, x=\begin{bmatrix}x_0&x_1\end{bmatrix}^T$ be a vector. The \emph{inner product} (also \emph{scalar product} or \emph{dot product}) of $x=\begin{bmatrix}x_0&x_1\end{bmatrix}$ and $y=\begin{bmatrix}y_0&y_1\end{bmatrix}$ is $\langle x,y\rangle=x_0y_0+x_1y_1\inlineeqnum\label{e:inner_product}$. $\langle x,x\rangle=x_0^2+x_1^2$, $\langle x,x\rangle\geq 0$, $x_0=x_1=0\Rightarrow\langle x,x\rangle =0$. The \emph{norm} is $\norm{x} = \sqrt{\langle x,x\rangle}\inlineeqnum$. A \emph{unit vector} $x$ has $\norm{x}=1$. Eq \ref{e:inner_product} depends on the coordinates axes. Let $\theta_x$ the angle between the positive horizontal axis and $x$. Define $\theta_y$ similarly. So, $\langle x,y\rangle = x_0y_0+x_1y_1 = (\norm{x}\cos\theta_x)(\norm{y}\cos\theta_y)+(\norm{x}\sin\theta_x)(\norm{y}\sin\theta_y) = \norm{x}\norm{y}(\cos\theta_x\cos\theta_y+\sin\theta_x\sin\theta_y)=\norm{x}\norm{y}(\cos\theta_x\cos-\theta_y-\sin\theta_x\sin-\theta_y)=\norm{x}\norm{y}\cos(\theta_x-\theta_y) = \norm{x}\norm{y}\cos\theta \inlineeqnum$, as $\cos\theta_y=\cos-\theta_y$, $\sin(\theta_y)=-\sin(-\theta_y)$ and $\cos(a+b)=\cos a\cos b-\sin a\sin b$. 
  
  For fixed vector norms, the greater the inner product, the closer the vectors are in orientation. $\langle x,y\rangle=0$ if $\norm{x}=0$ or $ \norm{y}=0 $ (one of them is $ \begin{bmatrix}0&0 \end{bmatrix}^T $) or $ \cos\theta=0 $ ($ \theta=\pm\pi/2 $), which means they are \emph{orthogonal} or \emph{perpendicular}. The \emph{distance} is the norm of the difference: $d(x,y)=\norm{x-y}=\sqrt{\langle x-y, x-y\rangle}=\sqrt{(x-y)^2+(x-y)^2}\inlineeqnum$.
  \section{oi}
  $lkj\inlineeqnum\label{e:adf}$
  
  Eq \ref{e:adf} shows
  
	\lipsum[1-30]
\end{document}